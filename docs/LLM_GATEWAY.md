# LLM Gateway - Endpoint Interno para Modelos Locales

## ðŸŽ¯ **PropÃ³sito**

El **LLM Gateway** es un endpoint interno (`/api/internal/llm-gateway`) que permite que los modelos locales (LLaMA 3 8B, Gemma 2B) puedan acceder al sistema RAG y Kimi sin modificar el frontend.

## ðŸ”„ **Flujo de Arquitectura**

```
LLaMA (local) â†’ /api/internal/llm-gateway â†’ Backend â†’ RAG/Kimi â†’ Cache â†’ Respuesta
```

- **Frontend**: Usa `/api/v1/chat` con switch Kimi/RAG
- **Modelos Locales**: Usan `/api/internal/llm-gateway` con routing automÃ¡tico

## ðŸ“¡ **Endpoints Disponibles**

### 1. Gateway Principal
```
POST /api/internal/llm-gateway
```

**Request:**
```json
{
  "query": "Â¿QuÃ© es la ortogonalidad?",
  "mode": "auto",      // "auto" | "kimi" | "rag"
  "session_id": 1
}
```

**Response:**
```json
{
  "answer": "Respuesta del sistema...",
  "mode_used": "rag",
  "cached": false,
  "timestamp": "2024-01-01T12:00:00Z"
}
```

### 2. Estado del Gateway
```
GET /api/internal/llm-gateway/status
```

**Response:**
```json
{
  "status": "operational",
  "cache_stats": {
    "total_cached": 150,
    "active_entries": 142
  },
  "timestamp": "2024-01-01T12:00:00Z"
}
```

## ðŸ§  **Modos de OperaciÃ³n**

### **Modo `auto`** (Recomendado)
El gateway decide automÃ¡ticamente usando heurÃ­sticas:

- **RAG** para preguntas tÃ©cnicas:
  - "quÃ© es", "cÃ³mo funciona", "explicar"
  - Consultas > 50 caracteres
  - Referencias a documentos/libros
  
- **Kimi** para conversaciÃ³n:
  - Saludos, preguntas cortas
  - "hola", "gracias", "tiempo"

### **Modo `rag`**
Fuerza el uso del sistema RAG con Gemini + PDFs.

### **Modo `kimi`**
Fuerza el uso de Kimi K2 (sin RAG).

## ðŸ’¾ **Sistema de Cache**

- **Base de datos**: SQLite (`llm_gateway_cache.db`)
- **TTL**: 24 horas por defecto
- **Hash**: SHA256 de `query + mode`
- **Beneficios**: Respuestas instantÃ¡neas para preguntas repetidas

## ðŸ›  **Uso con Modelos Locales**

### **OpciÃ³n 1: Script Python**
```bash
# BÃ¡sico
python src/scripts/local_llm_gateway_client.py "Â¿QuÃ© es la ortogonalidad?"

# EspecÃ­fico
python src/scripts/local_llm_gateway_client.py "hola" --mode kimi

# Interactivo
python src/scripts/local_llm_gateway_client.py --interactive
```

### **OpciÃ³n 2: Direct HTTP**
```python
import requests

def ask_backend(query, mode="auto"):
    payload = {"query": query, "mode": mode, "session_id": 1}
    response = requests.post(
        "http://localhost:8000/api/internal/llm-gateway",
        json=payload
    )
    return response.json()

# Uso
result = ask_backend("explicar Python")
print(result['answer'])
```

### **OpciÃ³n 3: Desde Ollama**
```bash
# Template para Ollama
curl -X POST http://localhost:8000/api/internal/llm-gateway \
  -H "Content-Type: application/json" \
  -d '{"query": "Â¿QuÃ© es DRY?", "mode": "auto", "session_id": 1}'
```

## ðŸŽ¯ **Casos de Uso TÃ­picos**

### **LLaMA 3 8B (Modelo Principal)**
```python
# Para preguntas complejas
response = ask_backend("explicar arquitectura microservicios", mode="rag")

# Para conversaciÃ³n
response = ask_backend("hola, cÃ³mo estÃ¡s?", mode="kimi")

# AutomÃ¡tico (recomendado)
response = ask_backend("Â¿QuÃ© significa ser pragmÃ¡tico?")
```

### **Gemma 2B (Modelo Ligero)**
```python
# Delega trabajo pesado al RAG
response = ask_backend("documentaciÃ³n Python requests", mode="rag")

# Consultas rÃ¡pidas
response = ask_backend("weather today", mode="kimi")
```

## ðŸ“Š **Ventajas del Sistema**

### **âœ… Sin Modificar Frontend**
- Usuarios no ven cambios
- Switch original intacto
- Experiencia consistente

### **âœ… Cache Inteligente**
- Respuestas instantÃ¡neas repetidas
- ReducciÃ³n de costos API
- Mejor rendimiento

### **âœ… Routing AutomÃ¡tico**
- HeurÃ­sticas inteligentes
- Balance RAG/Kimi Ã³ptimo
- Transparencia para usuario

### **âœ… Extensible**
- FÃ¡cil agregar nuevos modelos
- Modular y mantenible
- MÃ©tricas integradas

## ðŸ”§ **ConfiguraciÃ³n y Monitoreo**

### **Ver Estado**
```bash
curl http://localhost:8000/api/internal/llm-gateway/status
```

### **Limpiar Cache**
```bash
# OpciÃ³n futura: DELETE /api/internal/llm-gateway/cache
```

### **Logs**
```bash
# Los logs del gateway aparecen en:
# - Consola del backend
# - Archivos de log configurados
```

## ðŸš€ **Ejemplo Completo**

```python
#!/usr/bin/env python3
"""Ejemplo: LLaMA local usando el gateway."""

import requests
import json

def smart_assistant(query):
    """Asistente inteligente que delega al backend."""
    
    # Consultar al gateway
    response = requests.post(
        "http://localhost:8000/api/internal/llm-gateway",
        json={"query": query, "mode": "auto", "session_id": 1}
    )
    
    if response.status_code == 200:
        data = response.json()
        
        # LLaMA procesa la respuesta del backend
        backend_answer = data['answer']
        mode_used = data['mode_used']
        
        # LLaMA genera respuesta final
        final_response = f"SegÃºn {'RAG' if mode_used == 'rag' else 'Kimi'}: {backend_answer}"
        
        return final_response
    else:
        return "Error al consultar el backend"

# Uso
print(smart_assistant("Â¿QuÃ© es la ortogonalidad?"))
print(smart_assistant("hola, cÃ³mo estÃ¡s?"))
```

## ðŸŽ‰ **Resumen**

El **LLM Gateway** permite que tus modelos locales:
- âœ… Accedan al RAG mejorado con Gemini
- âœ… Usen Kimi para conversaciÃ³n
- âœ… Disfruten de cache inteligente
- âœ… Operen sin modificar el frontend
- âœ… Se integren fÃ¡cilmente con Ollama

**Â¡Listo para usar! Los modelos locales ahora pueden aprovechar todo el poder del backend** ðŸš€
