"""
Servicio de aplicaci√≥n de chat refactorizado con arquitectura hexagonal.

Este servicio usa SOLO puertos (interfaces) del dominio, sin dependencias
de implementaciones concretas (adapters).

Tipado estricto para mypy --strict con Python 3.12+
"""

from __future__ import annotations

import logging
import re
import time
import traceback
from datetime import UTC, datetime
from typing import TYPE_CHECKING

from src.adapters.db.document_mapper import DocumentMapper
from src.application.services.metrics_service import MetricsService
from src.application.services.rag_context_service import RagContextService
from src.domain.models import ChatMessageCreate, ChatSessionCreate, MessageRole

if TYPE_CHECKING:
    from src.application.services.embeddings_service_v2 import EmbeddingsServiceV2
    from src.domain.models import ChatMessage, ChatSession
    from src.domain.ports import ChatRepositoryPort, FileRepositoryPort, LLMPort
    from src.domain.ports.python_search_port import PythonSearchPort, PythonSource


logger = logging.getLogger(__name__)


class ChatServiceV2:
    """
    Servicio de aplicaci√≥n para chat siguiendo arquitectura hexagonal.

    Este servicio orquesta la l√≥gica de negocio sin conocer detalles
    de implementaci√≥n (Groq, Gemini, SQLite, PostgreSQL, etc.).

    Principios:
    - Depende SOLO de puertos (interfaces)
    - No importa de adapters
    - L√≥gica de negocio pura
    - F√°cil de testear con mocks
    """

    def __init__(
        self,
        llm_client: LLMPort,
        repository: ChatRepositoryPort,
        *,
        fallback_llm: LLMPort | None = None,
        embeddings_service: EmbeddingsServiceV2 | None = None,
        python_search: PythonSearchPort | None = None,
        metrics_service: MetricsService | None = None,
        file_repository: FileRepositoryPort | None = None,
    ) -> None:
        """
        Inicializa el servicio de chat.

        Args:
            llm_client: Cliente LLM principal (ej: Groq)
            repository: Repositorio de chat
            fallback_llm: Cliente LLM de respaldo (ej: Gemini)
            embeddings_service: Servicio de embeddings para RAG (opcional)
            python_search: Servicio de b√∫squeda Python (opcional)
            metrics_service: Servicio de m√©tricas (opcional)
            file_repository: Repositorio de archivos (opcional, para RAG context)
        """
        self.llm = llm_client
        self.repo = repository
        self.fallback_llm = fallback_llm
        self.embeddings = embeddings_service
        self.python_search = python_search

        # Servicios para gesti√≥n de contexto RAG
        self.context_service = None
        self.document_mapper = None
        if file_repository:
            self.context_service = RagContextService(file_repository)
            self.document_mapper = DocumentMapper(file_repository)

        # Servicio de m√©tricas (inyectado o crear uno por defecto)
        if metrics_service is None:
            # Importaci√≥n local para evitar dependencia circular
            from src.adapters.repositories.metrics_repository import (
                SQLModelMetricsRepository,
            )
            self.metrics = MetricsService(repository=SQLModelMetricsRepository())
        else:
            self.metrics = metrics_service

        # Almacenar √∫ltimas fuentes usadas para feedback
        self.last_search_sources: list[PythonSource] = []

    def create_session(self, session_data: ChatSessionCreate) -> ChatSession:
        """
        Crea una nueva sesi√≥n de chat.

        Args:
            session_data: Datos para crear la sesi√≥n

        Returns:
            Sesi√≥n creada
        """
        return self.repo.create_session(session_data)

    def get_session(self, session_id: str) -> ChatSession | None:
        """
        Obtiene una sesi√≥n por su ID.

        Args:
            session_id: ID de la sesi√≥n

        Returns:
            Sesi√≥n encontrada o None
        """
        return self.repo.get_session(session_id)

    def list_sessions(self, *, limit: int = 50) -> list[ChatSession]:
        """
        Lista las sesiones de chat.

        Args:
            limit: N√∫mero m√°ximo de sesiones

        Returns:
            Lista de sesiones
        """
        return self.repo.list_sessions(limit=limit)

    def create_session_from_user(self, user_id: str) -> ChatSession:
        """Crea una nueva sesi√≥n para un usuario con un t√≠tulo por defecto."""
        session_data = ChatSessionCreate(
            user_id=user_id,
            title=f"Chat {datetime.now(UTC).strftime('%Y-%m-%d %H:%M')}"
        )
        return self.create_session(session_data)

    def delete_session(self, session_id: str) -> bool:
        """Elimina una sesi√≥n de chat."""
        return self.repo.delete_session(session_id)

    def get_session_messages(self, session_id: str) -> list[ChatMessage]:
        """Obtiene todos los mensajes de una sesi√≥n."""
        return self.repo.get_session_messages(session_id)

    def list_sessions_for_user(self, user_id: str, limit: int = 50) -> list[dict]:
        """Lista las sesiones de un usuario con el conteo de mensajes."""
        all_sessions = self.repo.list_sessions(limit=limit * 2)
        user_sessions = [s for s in all_sessions if s.user_id == user_id]

        detailed_sessions = []
        for s in user_sessions[:limit]:
            message_count = self.repo.count_session_messages(str(s.id))
            detailed_sessions.append(
                {
                    "id": int(s.id),
                    "user_id": s.user_id,
                    "session_name": s.title if hasattr(s, 'title') else None,
                    "message_count": message_count,
                    "created_at": s.created_at.isoformat() if s.created_at else None,
                    "updated_at": s.updated_at.isoformat() if s.updated_at else None,
                }
            )
        return detailed_sessions


    def _resolve_target_file_id(
        self,
        session_id: str,
        user_message: str,
        provided_file_id: int | None
    ) -> int | None:
        """
        Resuelve qu√© archivo usar como contexto para RAG.

        Prioridad:
        1. ID expl√≠cito en el mensaje (ej: "ID:5")
        2. ID proporcionado por la UI (sidebar select)
        3. Referencia contextual (ej: "este documento") apuntando al √∫ltimo usado
        """
        # 1. Buscar menci√≥n expl√≠cita "ID:X" en el mensaje
        if self.document_mapper:
            explicit_id = self.document_mapper.parse_document_reference_from_text(user_message)
            if explicit_id:
                logger.info(f"üìÑ Detectada referencia expl√≠cita a archivo ID:{explicit_id}")
                return explicit_id

        # 2. Usar ID proporcionado por UI (si existe)
        if provided_file_id:
            return provided_file_id

        # 3. Verificar referencia contextual ("este pdf")
        if self.context_service and self.context_service.is_referencing_current_document(user_message):
            context_id = self.context_service.get_current_file_id(session_id)
            if context_id:
                logger.info(f"üìÑ Detectada referencia contextual a archivo ID:{context_id}")
                return context_id

        return None

    async def handle_message(
        self,
        session_id: str,
        user_message: str,
        *,
        agent_mode: str = "architect",
        file_id: int | None = None,
        max_tokens: int | None = None,
        temperature: float | None = None,
        use_fallback_on_error: bool = True,
        use_internet: bool = True,
    ) -> str:
        """
        Maneja un mensaje del usuario y retorna la respuesta del LLM.

        Args:
            session_id: ID de la sesi√≥n
            user_message: Mensaje del usuario
            agent_mode: Modo del agente (architect, code_generator, etc.)
            file_id: ID del archivo PDF para RAG (opcional)
            max_tokens: Tokens m√°ximos de respuesta
            temperature: Temperatura del modelo
            use_fallback_on_error: Si usar LLM de respaldo en caso de error
            use_internet: Si usar b√∫squeda en Internet

        Returns:
            Respuesta del LLM

        Raises:
            ValueError: Si la sesi√≥n no existe
        """
        # Iniciar timer para m√©tricas
        start_time = time.time()

        # --- RESOLUCI√ìN DE CONTEXTO RAG ---
        # Determinar el file_id real a usar
        resolved_file_id = self._resolve_target_file_id(session_id, user_message, file_id)

        # Si se resolvi√≥ un archivo, actualizar el contexto de la sesi√≥n
        if resolved_file_id:
            if self.context_service:
                self.context_service.set_current_file_id(session_id, resolved_file_id)
            # Usar el ID resuelto en lugar del proporcionado
            file_id = resolved_file_id
        # ----------------------------------

        rag_chunks_count = 0
        used_bear = False
        bear_sources_count = 0
        model_used = "kimi-k2"  # Default
        # 1. Validar o crear sesi√≥n
        if session_id == "0" or not session_id:
            # Crear nueva sesi√≥n si no existe
            session_data = ChatSessionCreate(
                user_id="streamlit_user",
                title=f"Chat {datetime.now(UTC).strftime('%Y-%m-%d %H:%M')}"
            )
            new_session = self.repo.create_session(session_data)
            session_id = str(new_session.id)
        else:
            # Validar que la sesi√≥n existe
            session = self.repo.get_session(session_id)
            if not session:
                raise ValueError(f"Sesi√≥n {session_id} no encontrada")

        # 2. Guardar mensaje del usuario
        user_msg_data = ChatMessageCreate(
            session_id=session_id,
            role=MessageRole.USER,
            content=user_message,
        )
        self.repo.add_message(user_msg_data)

        # 3. Obtener historial de mensajes
        history = self.repo.get_session_messages(session_id)

        # 4. Buscar contexto RAG si hay file_id
        rag_context = ""
        if file_id and self.embeddings:
            try:
                # üéØ B√öSQUEDA ADAPTATIVA: Ajustar top_k seg√∫n complejidad de la pregunta
                # Preguntas complejas necesitan m√°s contexto para aprovechar ventana de Gemini
                from src.adapters.config.settings import settings

                question_length = len(user_message)
                is_complex = any(word in user_message.lower() for word in [
                    # An√°lisis y comparaci√≥n
                    'compara', 'diferencia', 'diferencias', 'relaci√≥n', 'relaciona',
                    'contrasta', 'versus', 'vs', 'frente a', 'comparaci√≥n',
                    # Explicaci√≥n profunda
                    'explica detalladamente', 'explica en detalle', 'profundiza',
                    'desarrolla', 'elabora', 'detalla', 'describe en profundidad',
                    'extiende', 'ampl√≠a', 'expande',
                    # An√°lisis t√©cnico
                    'analiza', 'eval√∫a', 'examina', 'investiga', 'estudia',
                    'revisa', 'inspecciona', 'diagnostica',
                    # Enumeraci√≥n y listado
                    'enumera', 'lista', 'identifica', 'menciona todos',
                    'cu√°les son', 'qu√© tipos', 'qu√© clases',
                    # S√≠ntesis y conexi√≥n
                    'sintetiza', 'resume extensamente', 'conecta', 'vincula',
                    'integra', 'unifica', 'combina',
                    # Ejemplos y casos
                    'ejemplos', 'ejemplo pr√°ctico', 'casos de uso', 'casos pr√°cticos',
                    'demuestra', 'ilustra', 'muestra c√≥mo',
                    # Procedimientos y pasos
                    'paso a paso', 'procedimiento', 'proceso completo', 'c√≥mo hacer',
                    'implementar', 'aplicar en la pr√°ctica',
                    # Conceptos avanzados
                    'ventajas y desventajas', 'pros y contras', 'beneficios y limitaciones',
                    'implicaciones', 'consecuencias', 'impacto',
                    # Contexto t√©cnico (SQL, programaci√≥n)
                    'optimizaci√≥n', 'rendimiento', 'mejor pr√°ctica', 'mejores pr√°cticas',
                    'arquitectura', 'dise√±o', 'patrones', 'estrategias'
                ])

                # Ajustar top_k din√°micamente usando configuraci√≥n
                if is_complex or question_length > 100:
                    top_k = settings.rag_complex_top_k  # Preguntas complejas
                    limit = settings.rag_complex_limit
                    complexity = "compleja"
                elif question_length > 50:
                    top_k = settings.rag_normal_top_k  # Preguntas normales
                    limit = settings.rag_normal_limit
                    complexity = "normal"
                else:
                    top_k = settings.rag_simple_top_k  # Preguntas simples
                    limit = settings.rag_simple_limit
                    complexity = "simple"

                logger.info(f"üéØ B√∫squeda adaptativa ({complexity}): top_k={top_k}, limit={limit} chars")

                # Buscar chunks relevantes
                # B√∫squeda m√°s precisa para definiciones t√©cnicas
                results = await self.embeddings.search_similar(
                    query=user_message,
                    file_id=str(file_id),
                    top_k=8,  # M√°s chunks para mejor contexto
                    min_similarity=0.65,  # Umbral m√°s estricto
                )

                if results:
                    logger.info(f"‚úÖ RAG: {len(results)} chunks encontrados para file_id={file_id}")
                    acc = 0
                    parts: list[str] = []

                    for r in results:
                        remaining = limit - acc
                        if remaining <= 100:  # M√≠nimo para que valga la pena
                            break

                        # EmbeddingsServiceV2 retorna 'text', no 'content'
                        content = r.get('text', '')
                        chunk_idx = r.get('chunk_index', 0)
                        similarity = r.get('similarity', 0.0)

                        if not content:
                            logger.warning(f"‚ö†Ô∏è Chunk {chunk_idx} sin contenido: {r.keys()}")
                            continue

                        snippet = content[:remaining]
                        parts.append(f"[chunk {chunk_idx}, score={similarity:.3f}]\n{snippet}")
                        acc += len(snippet)

                    rag_context = "\n\n".join(parts)
                    rag_chunks_count = len(parts)  # Guardar para m√©tricas
                    model_used = "gemini-2.5-flash"  # RAG usa Gemini
                    logger.info(f"üìÑ Contexto RAG: {acc} caracteres de {rag_chunks_count} chunks")
                    logger.debug(f"üîç Preview contexto: {rag_context[:300]}...")
                else:
                    logger.warning(f"‚ö†Ô∏è RAG: No se encontraron chunks para file_id={file_id}")
            except Exception as e:
                logger.error(f"‚ùå Error en b√∫squeda RAG: {e}")
                logger.error(traceback.format_exc())

        # 5. Construir system prompt
        system_prompt = self._get_system_prompt(agent_mode)

        # Si hay contexto RAG, PRIORIZAR el contexto del PDF
        if rag_context:
            # Prompt PRECISO para respuestas literales y detalladas
            system_prompt = (
                f"Eres un asistente experto en an√°lisis t√©cnico de documentos. El usuario ha cargado el documento PDF (file_id={file_id}).\n\n"
                "**INSTRUCCIONES CR√çTICAS - RESPUESTAS PRECISAS:**\n"
                f"1. Tienes acceso COMPLETO al documento file_id={file_id}\n"
                "2. Responde con la DEFINICI√ìN EXACTA del texto, manteniendo t√©rminos t√©cnicos\n"
                "3. PRESERVA todos los detalles: figuras, n√∫meros, t√©rminos geom√©tricos, ejemplos\n"
                "4. Para definiciones, cita LITERALMENTE y luego explica si es necesario\n"
                "5. NUNCA simplifiques conceptos t√©cnicos - mant√©n la precisi√≥n del autor\n"
                "6. Si menciona figuras, n√∫meros espec√≠ficos o t√©rminos t√©cnicos, incl√∫yelos todos\n\n"
                f"--- CONTENIDO COMPLETO DEL DOCUMENTO file_id={file_id} ---\n\n"
                f"{rag_context}\n\n"
                "--- FIN DEL DOCUMENTO ---\n\n"
                "Responde bas√°ndote en este contenido con m√°xima precisi√≥n t√©cnica."
            )
            logger.debug(f"üéØ System prompt RAG: {len(system_prompt)} caracteres")

        # 6. Obtener respuesta inicial del LLM
        initial_response, tokens = await self._get_llm_response(
            system_prompt=system_prompt,
            history=history,
            max_tokens=max_tokens,
            temperature=temperature,
            session_id=session_id,
            agent_mode=agent_mode,
            use_fallback_on_error=use_fallback_on_error,
            has_rag=bool(rag_context),
        )

        # 7. Verificar si necesita b√∫squeda en Internet
        used_bear = False  # Inicializar variables antes del bloque condicional
        bear_sources_count = 0
        if use_internet and self.python_search and not rag_context:
            logger.info("üîç Verificando si necesita b√∫squeda web...")
            if self._should_search_internet(user_message, initial_response):
                logger.info("‚úÖ Kimi solicit√≥ b√∫squeda web. Activando Brave Search...")
                sources = await self._search_python_sources(user_message)
                if sources:
                    used_bear = True  # Marcar uso de Bear API
                    bear_sources_count = len(sources)  # Contar fuentes
                    self.last_search_sources = sources
                    context = self._build_internet_context(sources)

                    logger.info(f"üìö Contexto web construido: {len(context)} caracteres de {len(sources)} fuentes")

                    # Re-llamar al LLM con contexto adicional
                    # IMPORTANTE: Incluir el contexto DENTRO del system prompt para que Kimi lo vea como conocimiento base
                    enriched_prompt = (
                        f"{system_prompt}\n\n"
                        f"--- INFORMACI√ìN ACTUALIZADA DE INTERNET ---\n"
                        f"{context}\n"
                        f"--- FIN DE INFORMACI√ìN ACTUALIZADA ---\n\n"
                        f"INSTRUCCI√ìN CR√çTICA: Acabas de recibir informaci√≥n actualizada de fuentes confiables. "
                        f"Usa ESTA informaci√≥n para responder la pregunta del usuario. "
                        f"NO digas que no tienes informaci√≥n. "
                        f"Proporciona una respuesta completa bas√°ndote en el contexto actualizado que acabas de recibir."
                    )

                    logger.info("ü§ñ Re-llamando a Kimi con contexto web...")

                    # Usar el historial completo (el LLM ver√° el contexto actualizado en el system prompt)
                    response, tokens = await self._get_llm_response(
                        system_prompt=enriched_prompt,
                        history=history,  # Usar historial completo
                        max_tokens=max_tokens,
                        temperature=temperature,
                        session_id=session_id,
                        agent_mode=agent_mode,
                        use_fallback_on_error=use_fallback_on_error,
                        has_rag=bool(rag_context),
                    )
                    logger.info(f"‚úÖ Respuesta con contexto web generada: {len(response)} caracteres")
                else:
                    response = initial_response
            else:
                response = initial_response
        else:
            response = initial_response

        # 8. Guardar respuesta del asistente
        assistant_msg_data = ChatMessageCreate(
            session_id=session_id,
            role=MessageRole.ASSISTANT,
            content=response,
        )
        self.repo.add_message(assistant_msg_data)

        # 9. Registrar m√©tricas
        response_time = time.time() - start_time
        try:
            # Extraer tokens de la respuesta (manejar diferentes formatos)
            prompt_tokens = 0
            completion_tokens = 0

            if isinstance(tokens, dict):
                # Formato diccionario (algunos LLMs)
                prompt_tokens = tokens.get('prompt_tokens', 0)
                completion_tokens = tokens.get('completion_tokens', 0)
            elif isinstance(tokens, int):
                # Formato entero (Groq retorna total)
                # Estimar: ~70% completion, 30% prompt
                completion_tokens = int(tokens * 0.7)
                prompt_tokens = tokens - completion_tokens
            elif tokens is None:
                # Gemini no retorna tokens, estimar basado en longitud
                prompt_tokens = len(user_message) // 4  # ~4 chars por token
                completion_tokens = len(response) // 4
                logger.debug(f"‚ö†Ô∏è Tokens estimados (LLM no los proporciona): {prompt_tokens + completion_tokens}")
            else:
                # Caso inesperado, estimar
                prompt_tokens = len(user_message) // 4
                completion_tokens = len(response) // 4
                logger.warning(f"‚ö†Ô∏è Tipo de tokens inesperado: {type(tokens)}")

            self.metrics.record_agent_usage(
                session_id=session_id,
                agent_mode=agent_mode,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                response_time=response_time,
                model_name=model_used,
                has_rag_context=bool(rag_context),
                rag_chunks_used=rag_chunks_count,
                file_id=str(file_id) if file_id else None,
                used_bear_search=used_bear,
                bear_sources_count=bear_sources_count
            )
            logger.info(f"üìä M√©tricas registradas: {prompt_tokens + completion_tokens} tokens, {response_time:.2f}s, modelo={model_used}")
        except Exception as e:
            logger.error(f"‚ùå Error registrando m√©tricas: {e}")
            logger.error(traceback.format_exc())

        return response

    def _get_system_prompt(self, agent_mode: str) -> str:
        """
        Obtiene el system prompt para un modo de agente.

        Args:
            agent_mode: Modo del agente (puede ser el valor del enum o el nombre)

        Returns:
            System prompt
        """
        # Importar get_system_prompt de prompts.py
        from src.adapters.agents.prompts import AgentMode, get_system_prompt

        # Instrucci√≥n com√∫n sobre limitaciones de conocimiento (espec√≠fica para Kimi-K2)
        knowledge_cutoff = (
            "\n\n**REGLAS DE CONOCIMIENTO Y B√öSQUEDA WEB (Kimi-K2):**\n"
            "Tu conocimiento base cubre hasta Python 3.13 (inclusive) y enero 2025.\n\n"
            "**PARA PREGUNTAS T√âCNICAS DE PYTHON:**\n"
            "- Responde normalmente con tu conocimiento experto de Python\n"
            "- Mant√©n la calidad t√©cnica y precisi√≥n en arquitectura, c√≥digo y mejores pr√°cticas\n"
            "- Usa tu experiencia como arquitecto/software engineer senior\n\n"
            "**CUANDO NECESITES B√öSQUEDA WEB:**\n"
            "Si la pregunta requiere informaci√≥n m√°s actual o menciona expl√≠citamente:\n"
            "- Python 3.14, 3.15, 'dev', 'main branch', 'nightly'\n"
            "- Librer√≠as sin soporte estable o releases muy recientes\n"
            "- Eventos/noticias/releases posteriores a enero 2025\n"
            "- Preguntas sobre clima, noticias, informaci√≥n en tiempo real\n\n"
            "ENTONCES responde: \"Voy a buscar informaci√≥n actualizada sobre esto.\"\n\n"
            "**IMPORTANTE:** Prioriza siempre respuestas t√©cnicas de alta calidad. "
            "Usa b√∫squeda web solo cuando sea estrictamente necesario para informaci√≥n actualizada."
        )

        try:
            # Intentar obtener el prompt del m√≥dulo prompts.py
            # get_system_prompt acepta tanto enum como string
            base_prompt = get_system_prompt(agent_mode)
            # Agregar las reglas de conocimiento
            return base_prompt + knowledge_cutoff
        except (KeyError, ValueError) as e:
            # Fallback: usar prompt por defecto del arquitecto
            logger.warning(f"‚ö†Ô∏è No se encontr√≥ prompt para modo '{agent_mode}', usando Arquitecto por defecto. Error: {e}")
            base_prompt = get_system_prompt(AgentMode.PYTHON_ARCHITECT)
            return base_prompt + knowledge_cutoff

    def _should_search_internet(self, user_message: str, kimi_response: str) -> bool:
        """Detecta si Kimi no pudo resolver el problema y necesita b√∫squeda."""
        # Se√±al PRINCIPAL: La frase literal que Kimi debe decir seg√∫n el prompt actualizado
        if "voy a buscar informaci√≥n actualizada sobre esto" in kimi_response.lower():
            return True

        # Se√±al ANTERIOR (mantener compatibilidad)
        if "voy a buscarlo en internet" in kimi_response.lower():
            return True

        # Se√±ales AMPLIADAS de que Kimi no tiene la informaci√≥n
        uncertainty_signals = [
            r"\bno tengo informaci√≥n suficiente\b",  # Frase del prompt
            r"\bno (tengo|cuento con|puedo proporcionar)\b",
            r"\b(desconozco|ignoro|no estoy seguro)\b",
            r"\bno puedo\b",
            r"\bno tengo\b",
            r"\bno dispongo\b",
            r"\bpodr√≠as consultar\b",
            r"\bpodr√≠as buscar\b",
            r"\berror.*desconocido\b",
            r"\bno encuentro\b",
            r"\bno tengo acceso\b",
            r"\bno puedo ver\b",
            r"\bno disponible\b",
            r"\bcomo modelo de lenguaje\b",
            r"\bno tengo la capacidad\b",
            r"\bno puedo navegar\b",
            r"\bno puedo acceder\b",
        ]

        kimis_uncertain = any(
            re.search(pattern, kimi_response, re.IGNORECASE)
            for pattern in uncertainty_signals
        )

        # Si el usuario menciona GitHub, b√∫squeda o internet
        bool(
            re.search(r"\b(github|buscar|internet|repo|repositorio)\b", user_message, re.IGNORECASE)
        )

        # Si el usuario menciona un traceback o error espec√≠fico
        traceback_mentioned = bool(
            re.search(r"Traceback|Error|Exception", user_message, re.IGNORECASE)
        )

        # Si menciona arquitectura hexagonal o patrones espec√≠ficos
        bool(
            re.search(r"\b(arquitectura hexagonal|clean architecture|ports and adapters)\b", user_message, re.IGNORECASE)
        )

        # Si pregunta por una API espec√≠fica
        bool(
            re.search(r"\b(c√≥mo usar|c√≥mo funciona|ejemplo de|ejemplos de)\b.*\w+", user_message, re.IGNORECASE)
        )

        # Activar siempre que NO sea una consulta general rechazada
        is_general_query = bool(
            re.search(r"\b(clima|temperatura|hora|d√≥lar|euro|noticias|recetas)\b", user_message, re.IGNORECASE)
        )

        # Activar para preguntas sobre versiones, novedades y actualizaciones
        bool(
            re.search(r"\b(nueva versi√≥n|√∫ltima versi√≥n|actualizaci√≥n|lanzamiento|release)\b.*\bpython\b", user_message, re.IGNORECASE)
        )

        # MODO ULTRA ESTRICTO: SOLO buscar cuando Kimi expl√≠citamente dice "no s√©" O hay un error cr√≠tico
        # Priorizar la respuesta de Kimi sobre cualquier heur√≠stica de la pregunta
        return (kimis_uncertain or traceback_mentioned) and not is_general_query

        # MODO AGRESIVO (comentado): Busca tambi√©n en preguntas de API, versiones, etc.
        # return (kimis_uncertain or search_mentioned or traceback_mentioned or
        #         architecture_mentioned or api_question or version_question) and not is_general_query

    async def _search_python_sources(self, user_message: str) -> list[PythonSource]:
        """Ejecuta b√∫squeda Bear para cualquier pregunta Python v√°lida."""
        if not self.python_search:
            return []

        # Determinar tipo de b√∫squeda basado en el contenido

        if "Traceback" in user_message:
            return await self.python_search.search_python_bug(user_message)
        elif re.search(r"\b(c√≥mo usar|ejemplo|funciona)\b.*\w+\.\w+", user_message, re.IGNORECASE):
            api_match = re.search(r"(\w+)\.(\w+)", user_message)
            if api_match:
                module, attr = api_match.groups()
                return await self.python_search.search_python_api(module, attr)
        elif re.search(r"\b(nueva versi√≥n|√∫ltima versi√≥n|actualizaci√≥n|lanzamiento|release)\b.*\bpython\b", user_message, re.IGNORECASE):
            return await self.python_search.search_python_best_practice("latest python version release")

        # Default: b√∫squeda general para cualquier pregunta Python v√°lida
        return await self.python_search.search_python_best_practice(user_message)

    def _build_internet_context(self, sources: list[PythonSource]) -> str:
        """Construye el contexto para el LLM con las fuentes encontradas."""
        lines = []
        for source in sources:
            lines.append(f"üìö **{source.title}** ({source.source_type}, confiabilidad: {source.reliability}/10)")
            lines.append(f"üîó {source.url}")
            lines.append(f"üí° {source.snippet}")
            lines.append("")
        return "\n".join(lines)

    async def _get_llm_response(
        self,
        system_prompt: str,
        history: list,  # list[ChatMessage] pero evitamos import circular
        max_tokens: int | None,
        temperature: float | None,
        session_id: str,
        agent_mode: str,
        use_fallback_on_error: bool,
        has_rag: bool,
    ) -> tuple[str, int | None]:
        """Helper para obtener respuesta del LLM con l√≥gica de fallback."""
        # IMPORTANTE: Sistema h√≠brido
        # - RAG (con file_id) ‚Üí Gemini 2.5 (fallback_llm)
        # - Chat normal ‚Üí Kimi-K2 (llm)

        if has_rag and self.fallback_llm:
            # RAG: Usar Gemini (mejor para contextos largos)
            logger.info("ü§ñ Usando Gemini 2.5 para RAG")

            try:
                response, tokens = await self.fallback_llm.get_chat_completion(
                    system_prompt=system_prompt,
                    messages=history,
                    max_tokens=max_tokens or 2048,  # M√°s tokens para RAG
                    temperature=temperature or 0.3,
                )
                return response, tokens
            except Exception as e:
                logger.error(f"‚ùå Error en Gemini: {e}")
                raise
        else:
            # Chat normal: Usar Kimi-K2 (m√°s r√°pido)
            logger.info("ü§ñ Usando Kimi-K2 para chat normal")

            try:
                response, tokens = await self.llm.get_chat_completion(
                    system_prompt=system_prompt,
                    messages=history,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    session_id=session_id,
                    agent_mode=agent_mode,
                    use_cache=True,  # Cach√© solo para chat normal
                )
                return response, tokens
            except Exception as e:
                # Fallback a Gemini si Kimi falla
                if use_fallback_on_error and self.fallback_llm:
                    logger.warning(f"‚ö†Ô∏è Kimi-K2 fall√≥, usando Gemini como fallback. Error: {e}")
                    response, tokens = await self.fallback_llm.get_chat_completion(
                        system_prompt=system_prompt,
                        messages=history,
                        max_tokens=max_tokens,
                        temperature=temperature,
                    )
                    return response, tokens
                else:
                    raise
