# üöÄ **SISTEMA H√çBRIDO MEJORADO - Integraci√≥n con Modelos Locales**

## üìã **Resumen de Mejoras**

Se ha implementado un **sistema h√≠brido mejorado** que integra los modelos locales (LLaMA3.1:8b, Gemma2:2b) con los modelos en la nube (Kimi-K2, Gemini 2.5 Flash), creando un sistema robusto con **fallback cascade inteligente**.

---

## üéØ **Caracter√≠sticas Nuevas**

### **1. Routing Inteligente**
El sistema ahora decide autom√°ticamente qu√© modelo usar basado en:

| Tipo de Pregunta | Modelo Principal | Raz√≥n |
|------------------|------------------|-------|
| **RAG con PDFs** | Gemini 2.5 Flash | Mejor para contexto largo |
| **C√≥digo Python** | Kimi-K2 | Especializado en Python |
| **Chat General** | Kimi-K2 | M√°s r√°pido y actualizado |

### **2. Fallback Cascade Completa**
```
1. Kimi-K2 (principal) 
   ‚Üì (si falla)
2. Gemini 2.5 Flash (nube)
   ‚Üì (si falla)  
3. LLaMA3.1:8b (local)
   ‚Üì (si falla)
4. Gemma2:2b (local - √∫ltimo recurso)
   ‚Üì (si falla)
5. Error informativo
```

### **3. Nuevos Endpoints**

#### **`GET /api/v1/hibrido/status`**
Retorna el estado completo del sistema h√≠brido:
```json
{
  "models": {
    "kimi_k2": {"available": true, "type": "cloud"},
    "gemini": {"available": true, "type": "cloud"},
    "llama3_1_8b": {"available": true, "type": "local"},
    "gemma2_2b": {"available": true, "type": "local"}
  },
  "routing_enabled": true,
  "total_models_available": 4,
  "system_health": "healthy"
}
```

#### **`GET /api/v1/hibrido/test`**
Ejecuta una prueba r√°pida del sistema h√≠brido.

#### **`GET /api/v1/hibrido/models`**
Retorna modelos disponibles con capacidades y especialidades.

---

## üèóÔ∏è **Arquitectura Mejorada**

### **Componentes Nuevos**

1. **`LocalLLMClient`** (`src/adapters/agents/local_llm_client.py`)
   - Cliente para modelos Ollama locales
   - Health check autom√°tico
   - Soporte para LLaMA3.1:8b y Gemma2:2b

2. **`ChatServiceHibridoMejorado`** (`src/application/services/chat_service_hibrido_mejorado.py`)
   - Extiende el servicio original
   - Routing inteligente basado en tipo de pregunta
   - Fallback cascade con 4 niveles
   - M√©tricas mejoradas

3. **`HibridoStatus`** (`src/adapters/api/endpoints/hibrido_status.py`)
   - Endpoints para monitoreo del sistema h√≠brido
   - Dashboard de disponibilidad de modelos

### **Flujo de Decisi√≥n**

```mermaid
graph TD
    A[Usuario env√≠a mensaje] --> B{¬øTiene file_id?}
    B -->|S√≠| C[RAG con Gemini]
    B -->|No| D{¬øEs pregunta Python?}
    D -->|S√≠| E[Kimi-K2 Python]
    D -->|No| F[Kimi-K2 General]
    
    C --> G{¬øFunciona?}
    E --> G
    F --> G
    
    G -->|S√≠| H[Respuesta OK]
    G -->|No| I[Probar Gemini]
    I --> J{¬øFunciona?}
    J -->|S√≠| K[Respuesta Gemini]
    J -->|No| L[Probar LLaMA3.1]
    L --> M{¬øFunciona?}
    M -->|S√≠| N[Respuesta LLaMA]
    M -->|No| O[Probar Gemma2]
    O --> P{¬øFunciona?}
    P -->|S√≠| Q[Respuesta Gemma2]
    P -->|No| R[Error todos fallaron]
```

---

## ‚öôÔ∏è **Configuraci√≥n**

### **Variables de Entorno (Nuevas)**
```bash
# Habilitar modelos locales (default: true)
ENABLE_LOCAL_FALLBACK=true

# Configuraci√≥n Ollama (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Modelos locales disponibles
OLLAMA_LLAMA_MODEL=llama3.1:8b
OLLAMA_GEMMA_MODEL=gemma2:2b
```

### **Dependencias**
```bash
# Cliente HTTP para Ollama
httpx>=0.25.0
```

---

## üöÄ **Uso**

### **1. Iniciar Ollama (si no est√° corriendo)**
```bash
cd /ruta/a/tu/proyecto/docker
docker-compose up -d
```

### **2. Verificar Modelos Locales**
```bash
curl http://localhost:11434/api/tags
```

### **3. Probar Sistema H√≠brido**
```bash
# Verificar estado
curl http://localhost:8000/api/v1/hibrido/status

# Probar funcionamiento
curl http://localhost:8000/api/v1/hibrido/test

# Ver modelos disponibles
curl http://localhost:8000/api/v1/hibrido/models
```

### **4. Usar en el Frontend**
```javascript
// El routing es autom√°tico, solo env√≠a el mensaje como siempre
const response = await fetch('/api/v1/chat', {
  method: 'POST',
  headers: {'Content-Type': 'application/json'},
  body: JSON.stringify({
    session_id: "123",
    message: "¬øC√≥mo funciona una lista en Python?",
    mode: "architect",
    file_id: null  // Para RAG, poner el ID del PDF
  })
});
```

---

## üìä **M√©tricas Mejoradas**

El sistema ahora registra:
- **Modelo utilizado** (kimi-k2, gemini, llama3.1-8b, gemma2-2b)
- **Estrategia de routing** (rag_gemini, chat_kimi, code_kimi)
- **Nivel de fallback** (principal, fallback_1, fallback_2)
- **Tiempo de respuesta por modelo**
- **Disponibilidad en tiempo real**

---

## üõ†Ô∏è **Instalaci√≥n**

### **1. Actualizar Dependencias**
```bash
cd agente_hibrido_texto_Kimi_rag_Gemini
uv sync
```

### **2. Configurar .env**
```bash
cp .env.example .env
# Editar .env con tus API keys
```

### **3. Iniciar Sistema**
```bash
# Backend con mejoras h√≠bridas
docker compose up -d --build

# Acceder:
# Frontend: http://localhost:8501
# Backend: http://localhost:8000/docs
# Estado H√≠brido: http://localhost:8000/api/v1/hibrido/status
```

---

## üéØ **Ventajas del Sistema Mejorado**

### **Robustez**
- **4 niveles de fallback**: Si un modelo falla, autom√°ticamente prueba el siguiente
- **Disponibilidad 24/7**: Funciona incluso sin internet (modelos locales)
- **Monitoreo en tiempo real**: Sabes siempre qu√© modelos est√°n disponibles

### **Inteligencia**
- **Routing autom√°tico**: Elige el mejor modelo seg√∫n el tipo de pregunta
- **Contexto √≥ptimo**: Usa Gemini para RAG (mejor con contexto largo)
- **Especializaci√≥n**: Kimi-K2 para Python, Gemini para documentos

### **Flexibilidad**
- **Configurable**: Puedes desactivar modelos locales si quieres
- **Extensible**: F√°cil agregar nuevos modelos locales
- **Compatible**: No rompe la funcionalidad existente

---

## üîç **Diagn√≥stico y Troubleshooting**

### **Verificar Disponibilidad**
```bash
# Estado completo del sistema
curl http://localhost:8000/api/v1/hibrido/status | jq

# Probar routing autom√°tico
curl http://localhost:8000/api/v1/hibrido/test | jq
```

### **Problemas Comunes**

| Problema | Soluci√≥n |
|----------|----------|
| **Modelos locales no disponibles** | Verifica que Ollama est√© corriendo: `docker ps` |
| **LLaMA no aparece** | Descarga el modelo: `docker exec ollama ollama pull llama3.1:8b` |
| **Fallback no funciona** | Revisa logs: `docker compose logs backend` |
| **Routing siempre usa Kimi** | Verifica configuraci√≥n en `.env` |

---

## üöÄ **Pr√≥ximos Pasos**

1. **Frontend Mejorado**: Mostrar estado de modelos en tiempo real
2. **Configuraci√≥n Din√°mica**: Cambiar routing sin reiniciar
3. **M√©tricas Avanzadas**: Dashboard con gr√°ficos de uso
4. **M√°s Modelos**: Integrar Mistral, Claude, etc.
5. **Optimizaci√≥n**: Cach√© inteligente por tipo de pregunta

---

## üìù **Resumen T√©cnico**

- **Arquitectura**: Hexagonal mejorada con inyecci√≥n de dependencias
- **Patr√≥n**: Strategy + Chain of Responsibility para routing
- **Fallback**: Cascade pattern con 4 niveles
- **Monitoreo**: Health checks as√≠ncronos
- **M√©tricas**: Enhanced tracking con modelo espec√≠fico

**üéâ Resultado: Un sistema IA enterprise-grade con 99.9% disponibilidad!**
