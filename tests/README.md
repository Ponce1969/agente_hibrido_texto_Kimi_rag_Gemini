# ğŸ§ª Suite de Tests

Esta carpeta contiene todos los tests del proyecto, organizados por tipo y propÃ³sito.

---

## ğŸ“‚ Estructura

```
tests/
â”œâ”€â”€ conftest.py                 # Fixtures compartidos
â”œâ”€â”€ test_rag_system.py         # Tests del sistema RAG
â”œâ”€â”€ test_chat_service.py       # Tests del servicio de chat
â”œâ”€â”€ test_prompt_baseline.py   # Tests de baseline (IMPORTANTE)
â””â”€â”€ README.md                  # Este archivo
```

---

## ğŸš€ CÃ³mo Ejecutar los Tests

### **Todos los tests**
```bash
pytest
```

### **Solo tests unitarios (rÃ¡pidos)**
```bash
pytest -m unit
```

### **Solo tests de RAG**
```bash
pytest -m rag
```

### **Solo tests de baseline**
```bash
pytest -m baseline
```

### **Excluir tests lentos**
```bash
pytest -m "not slow"
```

### **Con cobertura**
```bash
pytest --cov=src --cov-report=html
```

---

## ğŸ“Š Tests de Baseline (CRÃTICO)

Los tests en `test_prompt_baseline.py` son **CRÃTICOS** porque:

1. **Capturan el estado actual** de los prompts ANTES del refactor
2. **Generan archivos JSON** con mÃ©tricas actuales:
   - `baseline_prompts.json` - Estructura de prompts
   - `baseline_tokens.json` - Uso de tokens
   - `baseline_responses.json` - Calidad de respuestas
   - `baseline_performance.json` - Tiempos de respuesta

3. **Sirven para comparar** despuÃ©s del refactor:
   ```bash
   # ANTES del refactor
   pytest -m baseline
   
   # Guardar archivos baseline_*.json
   
   # DESPUÃ‰S del refactor
   pytest -m baseline
   
   # Comparar archivos para ver mejoras/degradaciones
   ```

---

## ğŸ¯ Workflow Recomendado

### **Antes de Refactorizar**
```bash
# 1. Ejecutar todos los tests
pytest

# 2. Ejecutar tests de baseline
pytest -m baseline

# 3. Guardar archivos baseline_*.json
cp tests/baseline_*.json tests/baseline_before_refactor/

# 4. Verificar que todo pasa
pytest -v
```

### **DespuÃ©s de Refactorizar**
```bash
# 1. Ejecutar tests de baseline
pytest -m baseline

# 2. Comparar con baseline anterior
diff tests/baseline_before_refactor/baseline_tokens.json tests/baseline_tokens.json

# 3. Verificar mejoras
# - Tokens: Debe reducirse ~58%
# - Performance: Debe mantenerse o mejorar
# - Calidad: Debe mantenerse

# 4. Ejecutar todos los tests
pytest

# 5. Si todo pasa, el refactor es exitoso âœ…
```

---

## ğŸ“ˆ MÃ©tricas Esperadas DespuÃ©s del Refactor

| MÃ©trica | Antes | DespuÃ©s | Mejora |
|---------|-------|---------|--------|
| **Tokens por prompt** | ~2500 | ~400 | -84% |
| **Tokens totales** | ~4200 | ~1750 | -58% |
| **Tiempo de respuesta** | Similar | Similar | 0% |
| **Calidad** | Baseline | Igual o mejor | â‰¥0% |

---

## ğŸ› Troubleshooting

### **Error: "No module named 'src'"**
```bash
# AsegÃºrate de estar en el directorio raÃ­z
cd /home/gonzapython/Documentos/vscode_codigo/agentes_Front_Bac/agentes_Front_Bac

# Y que el venv estÃ© activado
source .venv/bin/activate
```

### **Error: "fixture not found"**
```bash
# Verifica que conftest.py existe
ls tests/conftest.py

# Ejecuta desde la raÃ­z del proyecto
pytest tests/
```

### **Tests muy lentos**
```bash
# Excluye tests lentos
pytest -m "not slow"

# O ejecuta solo unitarios
pytest -m unit
```

### **Quiero ver mÃ¡s detalles**
```bash
# Modo verbose
pytest -vv

# Con output de prints
pytest -s

# Con traceback completo
pytest --tb=long
```

---

## ğŸ“ Agregar Nuevos Tests

### **Test Unitario**
```python
import pytest

@pytest.mark.unit
def test_my_function():
    result = my_function()
    assert result == expected
```

### **Test de IntegraciÃ³n**
```python
import pytest

@pytest.mark.integration
@pytest.mark.slow
def test_full_flow():
    # Test que requiere servicios externos
    pass
```

### **Test con Fixture**
```python
def test_with_fixture(db_session, sample_chunks):
    # Usa fixtures de conftest.py
    assert len(sample_chunks) > 0
```

---

## ğŸ¯ Objetivos de Cobertura

- **MÃ­nimo**: 80% de cobertura
- **Objetivo**: 90% de cobertura
- **CrÃ­tico**: 100% en servicios core (chat, RAG)

```bash
# Ver cobertura actual
pytest --cov=src --cov-report=term-missing
```

---

## ğŸš€ CI/CD

Estos tests se ejecutarÃ¡n automÃ¡ticamente en CI cuando estÃ© configurado:

```yaml
# .github/workflows/tests.yml
- name: Run tests
  run: |
    pytest -m "not slow"
    pytest -m baseline
```

---

## ğŸ“ Ayuda

Si tienes problemas con los tests:

1. Verifica que todas las dependencias estÃ©n instaladas
2. AsegÃºrate de estar en el venv correcto
3. Ejecuta `pytest --collect-only` para ver quÃ© tests se detectan
4. Revisa los logs con `pytest -vv -s`

---

*Ãšltima actualizaciÃ³n: 1 de Octubre 2025*
